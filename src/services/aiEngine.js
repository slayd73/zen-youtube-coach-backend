// ======================================================// ü§ñ aiEngine.js ‚Äî FINAL 2025 (Groq + OpenAI)// Solo modelli realmente attivi e garantiti// ======================================================import dotenv from "dotenv";import Groq from "groq-sdk";import OpenAI from "openai";dotenv.config();const groqClient = process.env.GROQ_API_KEY  ? new Groq({ apiKey: process.env.GROQ_API_KEY })  : null;const openaiClient = process.env.OPENAI_API_KEY  ? new OpenAI({ apiKey: process.env.OPENAI_API_KEY })  : null;// Cleanerfunction clean(text) {  if (!text) return "";  return String(text)    .replace(/^```(json|txt)?/gi, "")    .replace(/```$/gi, "")    .trim();}// ======================================================// üß† MODEL 1: llama-3.2-90b-vision-preview (main)// ======================================================async function groqMain(prompt, options = {}) {  if (!groqClient) return null;  try {    const res = await groqClient.chat.completions.create({      model: "llama-3.2-90b-vision-preview",   // üî• MODELLO STABILE      messages: [{ role: "user", content: prompt }],      temperature: options.temperature ?? 0.25,      max_tokens: options.maxTokens ?? 5500    });    return clean(res.choices?.[0]?.message?.content);  } catch (err) {    console.error("‚ö†Ô∏è groqMain error:", err.message);    return null;  }}// ======================================================// üß† MODEL 2: llama-3.2-11b-vision-preview (secondary)// ======================================================async function groqSecondary(prompt, options = {}) {  if (!groqClient) return null;  try {    const res = await groqClient.chat.completions.create({      model: "llama-3.2-11b-vision-preview",      messages: [{ role: "user", content: prompt }],      temperature: options.temperature ?? 0.3,      max_tokens: options.maxTokens ?? 4500    });    return clean(res.choices?.[0]?.message?.content);  } catch (err) {    console.error("‚ö†Ô∏è groqSecondary error:", err.message);    return null;  }}// ======================================================// ‚ö° MODEL 3: llama-3.1-8b-instant (fast tasks)// ======================================================async function groqQuick(prompt, options = {}) {  if (!groqClient) return null;  try {    const res = await groqClient.chat.completions.create({      model: "llama-3.1-8b-instant",      messages: [{ role: "user", content: prompt }],      temperature: options.temperature ?? 0.35,      max_tokens: options.maxTokens ?? 3500    });    return clean(res.choices?.[0]?.message?.content);  } catch (err) {    return null;  }}// ======================================================// üü™ Backup MODEL: Mixtral// ======================================================async function groqBackup(prompt, options = {}) {  if (!groqClient) return null;  try {    const res = await groqClient.chat.completions.create({      model: "mixtral-8x7b-32768",      messages: [{ role: "user", content: prompt }],      temperature: options.temperature ?? 0.25,      max_tokens: options.maxTokens ?? 4500    });    return clean(res.choices?.[0]?.message?.content);  } catch (err) {    return null;  }}// ======================================================// ü§ñ OpenAI fallback (GPT-4o)// ======================================================async function openaiFallback(prompt, options = {}) {  if (!openaiClient) return null;  try {    const res = await openaiClient.chat.completions.create({      model: "gpt-4o",      messages: [{ role: "user", content: prompt }],      temperature: options.temperature ?? 0.25,      max_tokens: options.maxTokens ?? 5000    });    return clean(res.choices?.[0]?.message?.content);  } catch {    return null;  }}// ======================================================// üü¶ MASTER FUNCTION ‚Äî callAIModel// ======================================================export async function callAIModel(prompt, mode = "text", options = {}) {  const temperature = options.temperature ?? 0.25;  const maxTokens = options.maxTokens ?? 5000;  // üîä Audio (future use)  if (mode === "audioUrl") {    return "Audio transcription not implemented yet.";  }  // 1Ô∏è‚É£ Main model (90B)  let out = await groqMain(prompt, { temperature, maxTokens });  if (out && out.length > 50) return out;  // 2Ô∏è‚É£ Secondary model (11B)  out = await groqSecondary(prompt, { temperature, maxTokens });  if (out && out.length > 50) return out;  // 3Ô∏è‚É£ Quick model (8B)  out = await groqQuick(prompt, { temperature, maxTokens });  if (out && out.length > 50) return out;  // 4Ô∏è‚É£ Backup Mixtral  out = await groqBackup(prompt, { temperature, maxTokens });  if (out && out.length > 50) return out;  // 5Ô∏è‚É£ Fallback OpenAI GPT-4o  console.warn("‚ö†Ô∏è Groq fail ‚Üí fallback OpenAI");  out = await openaiFallback(prompt, { temperature, maxTokens });  return out || "";}