// =====================================================================//  ðŸ§  aiEngine.js â€” TOP BEST ULTRA-PREMIUM//  Motore AI centrale di Zen YouTube Coach Pro//  - Multi-provider: GROQ â†’ OPENAI â†’ LLAMA (via Groq)//  - Fallback automatico//  - Timeout di sicurezza//  - Logging avanzato// =====================================================================import dotenv from "dotenv";dotenv.config();// ----------------------------------------------------------//  ENV & Config// ----------------------------------------------------------const AI_PROVIDER = process.env.AI_PROVIDER || "groq"; // "groq" | "openai" | "llama"const OPENAI_API_KEY = process.env.OPENAI_API_KEY || null;const GROQ_API_KEY = process.env.GROQ_API_KEY || null;// Modelli di default (puoi cambiarli dal .env)const DEFAULT_OPENAI_MODEL =  process.env.OPENAI_MODEL || "gpt-4.1-mini";const DEFAULT_GROQ_MODEL =  process.env.GROQ_MODEL || "llama-3.1-70b-versatile";const DEFAULT_LLAMA_MODEL =  process.env.LLAMA_MODEL || "llama-3.1-70b-versatile";// Timeout massimo per ogni chiamata (ms)const AI_TIMEOUT_MS = Number(process.env.AI_TIMEOUT_MS || 45000);// ----------------------------------------------------------//  Utility: timeout con AbortController// ----------------------------------------------------------async function withTimeout(promise, ms, contextLabel = "AI call") {  const controller = new AbortController();  const id = setTimeout(() => controller.abort(), ms);  try {    const result = await promise(controller.signal);    clearTimeout(id);    return result;  } catch (err) {    clearTimeout(id);    if (err.name === "AbortError") {      throw new Error(`Timeout durante ${contextLabel}`);    }    throw err;  }}// ----------------------------------------------------------//  Provider: OpenAI (API compatibile /v1/chat/completions)// ----------------------------------------------------------async function callOpenAI(prompt, options = {}, signal) {  if (!OPENAI_API_KEY) {    throw new Error("OPENAI_API_KEY non configurata.");  }  const {    model = DEFAULT_OPENAI_MODEL,    temperature = 0.3,    maxTokens = 1200,    systemPrompt = "Sei un assistente esperto di YouTube e contenuti per pubblico Over50/60/70.",    jsonMode = false,  } = options;  const body = {    model,    messages: [      { role: "system", content: systemPrompt },      { role: "user", content: prompt },    ],    temperature,    max_tokens: maxTokens,  };  if (jsonMode) {    // Alcuni modelli supportano il response_format    body.response_format = { type: "json_object" };  }  const response = await fetch("https://api.openai.com/v1/chat/completions", {    method: "POST",    signal,    headers: {      "Content-Type": "application/json",      Authorization: `Bearer ${OPENAI_API_KEY}`,    },    body: JSON.stringify(body),  });  if (!response.ok) {    const text = await response.text();    throw new Error(      `Errore OpenAI (${response.status}): ${text || response.statusText}`    );  }  const data = await response.json();  const choice = data.choices?.[0]?.message?.content;  if (!choice) {    throw new Error("Risposta OpenAI senza contenuto.");  }  return choice;}// ----------------------------------------------------------//  Provider: Groq (compatibile OpenAI-style)// ----------------------------------------------------------async function callGroq(prompt, options = {}, signal) {  if (!GROQ_API_KEY) {    throw new Error("GROQ_API_KEY non configurata.");  }  const {    model = DEFAULT_GROQ_MODEL,    temperature = 0.3,    maxTokens = 1200,    systemPrompt = "Sei un analista YouTube per pubblico Over50/60/70, molto preciso e conciso.",    jsonMode = false,  } = options;  const body = {    model,    messages: [      { role: "system", content: systemPrompt },      { role: "user", content: prompt },    ],    temperature,    max_tokens: maxTokens,  };  // Groq al momento non richiede esplicitamente response_format,  // ma se un domani lo farÃ , siamo giÃ  pronti.  if (jsonMode) {    body.response_format = { type: "json_object" };  }  const response = await fetch(    "https://api.groq.com/openai/v1/chat/completions",    {      method: "POST",      signal,      headers: {        "Content-Type": "application/json",        Authorization: `Bearer ${GROQ_API_KEY}`,      },      body: JSON.stringify(body),    }  );  if (!response.ok) {    const text = await response.text();    throw new Error(      `Errore Groq (${response.status}): ${text || response.statusText}`    );  }  const data = await response.json();  const choice = data.choices?.[0]?.message?.content;  if (!choice) {    throw new Error("Risposta Groq senza contenuto.");  }  return choice;}// ----------------------------------------------------------//  Provider: LLaMA (via Groq, modello dedicato)// ----------------------------------------------------------async function callLlama(prompt, options = {}, signal) {  // Riutilizziamo Groq con un modello LLaMA dedicato  const llamaOptions = {    ...options,    model: options.model || DEFAULT_LLAMA_MODEL,  };  return callGroq(prompt, llamaOptions, signal);}// ----------------------------------------------------------//  Strategia di fallback provider// ----------------------------------------------------------// In base a AI_PROVIDER, definiamo lâ€™ordine con cui provare i provider.function getProviderPriority() {  const base = (AI_PROVIDER || "").toLowerCase().trim();  switch (base) {    case "openai":      return ["openai", "groq", "llama"];    case "llama":      return ["llama", "groq", "openai"];    case "groq":    default:      return ["groq", "openai", "llama"];  }}// ----------------------------------------------------------//  Funzione centrale: callAIModel//  - prompt: stringa obbligatoria//  - options: { model, temperature, maxTokens, systemPrompt, jsonMode }// ----------------------------------------------------------export async function callAIModel(prompt, options = {}) {  if (!prompt || typeof prompt !== "string") {    throw new Error("Prompt mancante o non valido in callAIModel.");  }  const providers = getProviderPriority();  const errors = [];  for (const provider of providers) {    try {      console.log(        `[AI Engine] Provo provider: ${provider.toUpperCase()} (modello: ${          options.model ||          (provider === "openai"            ? DEFAULT_OPENAI_MODEL            : provider === "groq"            ? DEFAULT_GROQ_MODEL            : DEFAULT_LLAMA_MODEL)        })`      );      const result = await withTimeout(        (signal) => {          if (provider === "openai") {            return callOpenAI(prompt, options, signal);          }          if (provider === "groq") {            return callGroq(prompt, options, signal);          }          if (provider === "llama") {            return callLlama(prompt, options, signal);          }          throw new Error(`Provider AI sconosciuto: ${provider}`);        },        AI_TIMEOUT_MS,        `chiamata ${provider}`      );      console.log(`[AI Engine] Provider ${provider.toUpperCase()} OK.`);      return result;    } catch (err) {      console.error(        `[AI Engine] Errore provider ${provider.toUpperCase()}:`,        err.message      );      errors.push({        provider,        error: err.message,      });      // prova il provider successivo    }  }  // Se siamo qui, tutti i provider sono falliti  console.error("[AI Engine] Tutti i provider sono falliti:", errors);  throw new Error(    `Tutti i provider AI hanno fallito. Dettagli: ${JSON.stringify(      errors,      null,      2    )}`  );}